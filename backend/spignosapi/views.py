from rest_framework.views import APIView
from rest_framework.response import Response
from rest_framework import status
from django.shortcuts import get_object_or_404, render
from django.views.decorators.csrf import csrf_exempt
from django.http import JsonResponse
from django.contrib.auth.models import User
from django.utils.crypto import get_random_string
from datetime import datetime

from .api.models import Conversation, Message
from .api.serializers import MessageSerializer
from .llm.handler import LLMHandler

from sentence_transformers import SentenceTransformer
import faiss
import numpy as np

from spignosapi.llm.handler import UnifiedLLMHandler

llm_handler = UnifiedLLMHandler(use_openai=True)


# âš™ï¸ ModÃ¨le d'embedding pour la recherche sÃ©mantique
embedding_model = SentenceTransformer("all-MiniLM-L6-v2")

# âš™ï¸ Base de connaissances simulÃ©e pour la partie RAG
knowledge_base = [
    "L'IA est utile pour l'automatisation des tÃ¢ches.",
    "Un modÃ¨le LLM peut Ãªtre utilisÃ© pour la gÃ©nÃ©ration de texte.",
    "Le RAG combine un modÃ¨le LLM et une base de recherche pour gÃ©nÃ©rer des rÃ©ponses plus prÃ©cises.",
]

# âš™ï¸ Indexation FAISS pour recherche vectorielle
knowledge_vectors = np.array(
    [embedding_model.encode(text) for text in knowledge_base], dtype="float32"
)
index = faiss.IndexFlatL2(knowledge_vectors.shape[1])
index.add(knowledge_vectors)


def home_view(request):
    return render(request, "home.html", {"year": datetime.now().year})


def retrieve_information(query: str) -> str:
    """
    ğŸ” Recherche d'informations pertinentes via FAISS dans la base simulÃ©e.
    """
    query_vector = np.array([embedding_model.encode(query)], dtype="float32")
    distances, indices = index.search(query_vector, k=1)

    if distances[0][0] < 1.0:  # Seuil configurable
        return knowledge_base[indices[0][0]]

    return "Pas d'information pertinente trouvÃ©e."


@csrf_exempt
def chat_page(request):
    """
    ğŸŒ Vue frontend : gÃ¨re la session utilisateur et affiche la page HTML.
    """
    if not request.session.get("user_id"):
        # ğŸ” CrÃ©ation dâ€™un utilisateur anonyme
        username = f"anon_{get_random_string(8)}"
        user = User.objects.create_user(username=username)
        request.session["user_id"] = user.id
    else:
        user = get_object_or_404(User, id=request.session["user_id"])

    if request.method == "POST":
        prompt = request.POST.get("prompt", "")
        output = llm_handler.generate(prompt)
        return render(request, "chat.html", {"prompt": prompt, "response": output})

    return render(request, "chat.html")


class ChatAPI(APIView):
    """
    ğŸ§  API REST pour gÃ©nÃ©rer une rÃ©ponse Ã  un prompt utilisateur
    avec une Ã©tape RAG (retrieval + gÃ©nÃ©ration).
    """

    def post(self, request, conversation_id=None):
        user_input = request.data.get("text", "")

        if not user_input:
            return Response({"error": "Message vide"}, status=status.HTTP_400_BAD_REQUEST)

        # ğŸ¯ RÃ©cupÃ©ration ou crÃ©ation de conversation
        if conversation_id:
            conversation = get_object_or_404(Conversation, id=conversation_id)
        else:
            conversation = Conversation.objects.create(
                user=request.user
            )  # Ã  adapter si pas d'authentification

        # ğŸ” Recherche d'information RAG
        retrieved_info = retrieve_information(user_input)

        # ğŸ§  CrÃ©ation du prompt enrichi
        prompt = f"Info utile : {retrieved_info}\nQuestion : {user_input}"

        # ğŸ’¬ GÃ©nÃ©ration de la rÃ©ponse
        response_text = llm_handler.generate(prompt)

        # ğŸ’¾ Sauvegarde dans la base
        message = Message.objects.create(
            conversation=conversation, text=user_input, response=response_text
        )

        return Response(MessageSerializer(message).data, status=status.HTTP_201_CREATED)
